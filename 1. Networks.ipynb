{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSS Lab: Networks\n",
    "This lab demonstrates how to use common tools to analyze the structure of several networks. Topics covered include visualization, centrality measures, shortest paths, and affiliation networks. At the end of the lab, you will be able to visualze and perform an analysis of your own social network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Setup](#Setup)\n",
    "2. [Build and visualize a network](#Build-and-visualize-a-network)\n",
    "    1. [Load the network](#Load-the-network)\n",
    "    2. [Visualize the network](#Visualize-the-network)\n",
    "3. [Centrality measures](#Centrality-measures)\n",
    "    1. [Find centrality](#Find-centrality)\n",
    "    2. [Interpret centrality](#Interpret-centrality)\n",
    "    3. [Compare centrality measures](#Compare-centrality-measures)\n",
    "7. [Your social network](#Your-social-network)\n",
    "    1. [Visualize your social network](#Visualize-your-social-network)\n",
    "    3. [Social network centrality](#Social-network-centrality)\n",
    "8. [References](#References)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "We import several libraries, including `networkx` for network algorithms, `pandas` for data processing, and `visJS2jupyter` for visualization. We also define some helper functions used by several of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import urllib.request\n",
    "import networkx as nx\n",
    "import networkx as networkx\n",
    "import networkx.algorithms as nxalg\n",
    "import networkx.algorithms.community as nxcom\n",
    "import networkx.readwrite as nxrw\n",
    "import pandas as pd\n",
    "import visJS2jupyter.visJS_module as vjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# Standardize a vector to 0 mean and unit variance\n",
    "def scale(v):\n",
    "    return (v - v.mean()) / v.std()\n",
    "\n",
    "# Return a data frame sorted by a node's centrality relative the mean of its centralities\n",
    "def rel_centrality(df, measure, labels=dict()):\n",
    "    name = 'rel_{}'.format(measure)\n",
    "    return pd.DataFrame({\n",
    "        name: scale(df[measure]) / df['mean_centrality'],\n",
    "        \"label\": [labels.get(x, x) for x in df.index]\n",
    "    })\n",
    "\n",
    "def get_colors():\n",
    "    phi = (1 + math.sqrt(5)) / 2\n",
    "    color = []\n",
    "    for i in range(1, 20):\n",
    "        theta = phi * i * math.pi * 2\n",
    "        x = 128 + math.floor(64*math.sin(theta))\n",
    "        y = 128 + math.floor(64*math.cos(theta))\n",
    "        color.append((x, x, y))\n",
    "    return color\n",
    "\n",
    "def visualize_visjs(\n",
    "        G, communities=None, colors=None, default_color=\"192,192,192\",\n",
    "        node_size_field=\"node_size\", layout=\"spring\", scale=500, pos=None,\n",
    "        groups=None, weight=None, labels=dict(), title=\"\"):\n",
    "    # Get list of nodes and edges\n",
    "    nodes = list(G.nodes())\n",
    "    edges = list(G.edges())\n",
    "    # Change node shapes for bipartite graph\n",
    "    if groups is None:\n",
    "        node_shapes = dict()\n",
    "        node_sizes = dict()\n",
    "        node_colors = dict()\n",
    "    else:\n",
    "        node_shapes = dict((n, \"square\") for n in groups)\n",
    "        node_sizes = dict((n, 15) for n in groups)\n",
    "        node_colors = dict((n, \"192,128,0\") for n in groups)\n",
    "    # Per-node properties\n",
    "    nodes_dict = dict((n, {\n",
    "        \"id\": labels.get(n, n),\n",
    "        \"node_size\": node_sizes.get(n, 5),\n",
    "        \"node_shape\": node_shapes.get(n, \"dot\")\n",
    "        }) for n in nodes)\n",
    "    # Generate a layout for the nodes\n",
    "    edge_smooth_enabled = False\n",
    "    edge_width = 4\n",
    "    edge_arrow_scale = 2\n",
    "    if communities is not None and pos is None:\n",
    "        # Generate initial positions based on community\n",
    "        phi = 3.14 / len(nodes)\n",
    "        community_node = []\n",
    "        # Create list of nodes and their communities\n",
    "        for i, com in enumerate(sorted(communities, key=lambda x: len(x), reverse=True)):\n",
    "            for node in com:\n",
    "                community_node.append((i, node))\n",
    "        # Sort by community and\n",
    "        community_node = sorted(community_node)\n",
    "        # Generate initial position by placing communities around a circle\n",
    "        pos = dict((d[1], (math.cos(i*phi), math.sin(i*phi))) for i, d in enumerate(community_node))\n",
    "    else:\n",
    "        pos = None\n",
    "    if layout == \"circle\":\n",
    "        pos = nx.circular_layout(G, scale=scale)\n",
    "    elif layout == \"spring\":\n",
    "        pos = nx.spring_layout(G, k=3/math.sqrt(len(nodes)), scale=scale, pos=pos)\n",
    "    else:\n",
    "        edge_smooth_enabled = True\n",
    "    # Assign position\n",
    "    for n in nodes:\n",
    "        nodes_dict[n][\"x\"] = pos[n][0]\n",
    "        nodes_dict[n][\"y\"] = pos[n][1]\n",
    "    # Calculate bounds for scaling\n",
    "    x_min = min(pos.values(), key=lambda x: x[0])[0]\n",
    "    x_max = max(pos.values(), key=lambda x: x[0])[0]\n",
    "    y_min = min(pos.values(), key=lambda x: x[1])[1]\n",
    "    y_max = max(pos.values(), key=lambda x: x[1])[1]\n",
    "    x_range = x_max - x_min\n",
    "    y_range = y_max - y_min\n",
    "    max_range = max(x_range, y_range)\n",
    "    # If we have communities, assign color based on community\n",
    "    if colors is None:\n",
    "        colors = [\"{},{},{}\".format(*c) for c in get_colors()]\n",
    "    if communities is not None:\n",
    "        for i, com in enumerate(sorted(communities, key=lambda x: len(x), reverse=True)):\n",
    "            for node in com:\n",
    "                try:\n",
    "                    nodes_dict[node][\"color\"] = \"rgba({},1)\".format(colors[i])\n",
    "                    nodes_dict[node][\"color_index\"] = i\n",
    "                except IndexError:\n",
    "                    nodes_dict[node][\"color\"] = \"rgba({},1)\".format(default_color)\n",
    "    # Update color for bipartite nodes\n",
    "    for node, node_attr in nodes_dict.items():\n",
    "        if node in node_colors:\n",
    "            node_attr[\"color\"] = \"rgba({},1)\".format(node_colors[node])\n",
    "    # Map node labels to contiguous ids\n",
    "    node_map = dict(zip(nodes,range(len(nodes))))\n",
    "    # Determine edge colors\n",
    "    edge_colors_idx = {}\n",
    "    for source, target in edges:\n",
    "        source_color = nodes_dict[source].get(\"color_index\", None)\n",
    "        target_color = nodes_dict[target].get(\"color_index\", None)\n",
    "        if source_color == target_color and source_color is not None:\n",
    "            edge_colors_idx[(source, target)] = source_color\n",
    "    edge_colors = dict(\n",
    "        (e,colors[c])\n",
    "        for e, c in edge_colors_idx.items() if c < len(colors))\n",
    "    # Per-edge properties, use contiguous ids to identify nodes\n",
    "    edge_scale = math.ceil(max_range / 200)\n",
    "    edges_dict = []\n",
    "    for source, target, data in G.edges(data=True):\n",
    "        edge = {\n",
    "            \"source\": node_map[source],\n",
    "            \"target\": node_map[target],\n",
    "            \"title\":'test',\n",
    "            \"color\": \"rgba({},0.3)\".format(edge_colors.get((source,target), default_color)),\n",
    "            \"edge_width_field\": \"value\",\n",
    "            \"value\": data.get(\"value\", 1) * edge_scale\n",
    "        }\n",
    "        edges_dict.append(edge)\n",
    "    # Convert nodes dict to node list\n",
    "    nodes_list = [nodes_dict[n] for n in nodes]\n",
    "    # Check for directed graph\n",
    "    if G.__class__ == nx.classes.digraph.DiGraph:\n",
    "        directed = True\n",
    "    else:\n",
    "        directed = False\n",
    "    # Call visjs\n",
    "    return vjs.visjs_network(\n",
    "        nodes_list, edges_dict,\n",
    "        node_size_field=\"node_size\",\n",
    "        node_size_multiplier=10.0,\n",
    "        edge_width_field=\"value\",\n",
    "        edge_width=edge_width,\n",
    "        edge_arrow_to=directed,\n",
    "        edge_arrow_to_scale_factor=edge_arrow_scale,\n",
    "        edge_smooth_enabled=edge_smooth_enabled,\n",
    "        edge_smooth_type=\"curvedCW\",\n",
    "        graph_id=hash(title))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build and visualize a network\n",
    "\n",
    "This section loads network data from a file and explores its basic properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def load_hamilton_affiliation():\n",
    "    B = nxrw.adjlist.read_adjlist(\"external/hamilton.csv\", delimiter=\"; \", comments=\"%\")\n",
    "    return B\n",
    "\n",
    "def load_hamilton(threshold=None):\n",
    "    # Load the song-character affiliation network\n",
    "    B = load_hamilton_affiliation()\n",
    "    # Get list of songs from the file\n",
    "    songs = set()\n",
    "    with open(\"external/hamilton.csv\") as f:\n",
    "        f.readline()\n",
    "        for row in f:\n",
    "            songs.add(row.split(\"; \")[0])\n",
    "    # Deduce list of charactes\n",
    "    characters = set(B.nodes()) - songs\n",
    "    # Project the affiliation network onto the set of characters\n",
    "    G = nxalg.bipartite.projection.weighted_projected_graph(B, characters)\n",
    "    # Threshold\n",
    "    if threshold is not None:\n",
    "        for s, t, data in list(G.edges(data=True)):\n",
    "            if data[\"weight\"] < threshold:\n",
    "                G.remove_edge(s,t)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load the network\n",
    "\n",
    "The next cell loads data from a file using the `networkx` library,\n",
    "and displays a list of nodes in the network.\n",
    "This example uses characters from the play _Hamilton_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_hamilton = load_hamilton()\n",
    "sorted(G_hamilton.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know the labels of the nodes, you can see which nodes are connected by an edge.\n",
    "In this case, two nodes are connected by an edge if the corresponding characters have parts in at least one song together.\n",
    "The next cell chooses a single node and prints a list of all the other nodes it's connected to.\n",
    "These nodes are called its neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(G_hamilton.neighbors('E. Schuyler'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualize the network\n",
    "In these visualizations, each circle represents a node.\n",
    "Edges between two nodes are represented by drawing a line between them.\n",
    "\n",
    "There are many ways to draw a network.\n",
    "One simple way is to space all the nodes evenly around a circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_visjs(G_hamilton, layout=\"circle\", title=\"Circular Layout Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common way to visualize a network is using a \"force-directed\" layout.\n",
    "In a force-directed layout, nodes push away from each other, but edges act like springs pulling them back together.\n",
    "As a result, nodes with many neighbors in common are pulled closer to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_visjs(G_hamilton, scale=2000, title=\"Force-Directed Layout Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Answer Q1\n",
    "In a few sentences, compare the benefits and drawbacks of circular and force-directed layouts. Consider large and small networks. Also consider which types of questions you can/can't answer with each layout.\n",
    "\n",
    "(answer here ~150 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Centrality measures\n",
    "\n",
    "One benefit of representing data as a network is that the patterns of connections between nodes can reveal useful information.\n",
    "Many standard techniques for investigating the structure of networks have been developed.\n",
    "\n",
    "One of the simplest questions to ask is: which nodes are most important?\n",
    "But what does \"important\" mean exactly?\n",
    "There are several common ways to measure importance, or _centrality_, of nodes in a nework.\n",
    "This section examines several of the most popular.\n",
    "\n",
    "While the _Hamilton_ network in the previous section is simple enough to be illustrative,\n",
    "a historical data set better demonstrates how centrality measures can be applied to real-world networks.\n",
    "This section uses historical data on affilations between organizers of the American Revolution,\n",
    "taken from [Using Metadata to Find Paul Revere](https://kieranhealy.org/blog/archives/2013/06/09/using-metadata-to-find-paul-revere/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def load_revere_affiliation(\n",
    "        url=\"https://raw.githubusercontent.com/kjhealy/revere/master/data/PaulRevereAppD.csv\"):\n",
    "    data = urllib.request.urlopen(url)\n",
    "    df = pd.read_csv(data).set_index(\"Unnamed: 0\")\n",
    "    people = list(df.index)\n",
    "    groups = list(df.columns)\n",
    "    #\n",
    "    B = nx.Graph()\n",
    "    for column in df.columns:\n",
    "        for row in df[df[column] == 1].index:\n",
    "            B.add_edge(column, row)\n",
    "    return people, groups, B\n",
    "\n",
    "def load_revere(\n",
    "        url=\"https://raw.githubusercontent.com/kjhealy/revere/master/data/PaulRevereAppD.csv\",\n",
    "        threshold=None, dual=False):\n",
    "    # Load the affiliation network\n",
    "    people, groups, B = load_revere_affiliation(url)\n",
    "    # Project the affiliation network onto the set of people\n",
    "    if dual:\n",
    "        G = nxalg.bipartite.projection.weighted_projected_graph(B, groups)\n",
    "    else:\n",
    "        G = nxalg.bipartite.projection.weighted_projected_graph(B, people)\n",
    "    # Threshold\n",
    "    if threshold is not None:\n",
    "        for s, t, data in list(G.edges(data=True)):\n",
    "            if data[\"weight\"] < threshold:\n",
    "                G.remove_edge(s,t)\n",
    "    # Copy \"weight\" data to \"value\" needed by visjs\n",
    "    w = nx.get_edge_attributes(G, \"weight\")\n",
    "    nx.set_edge_attributes(G, w, \"value\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Loading the network\n",
    "The next cell begins by loading the data into a `networkx` graph. As is, an analysis of the data will reveal Paul Revere to be highly central by all measures [Healey2013] so to make the example more interesting, let's ask this: if Paul Revere had been on vacation, who might have notified the Massachusetts Provincial Congress of the impending attack in his place? To do so, we remove Paul Revere from the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_revere = load_revere()\n",
    "people, groups, B_revere = load_revere_affiliation()\n",
    "G_revere.remove_node('Revere.Paul')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualizing the network\n",
    "The next cell visualizes the network. You can zoom in and out and hover over nodes to see their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_visjs(G_revere, scale=1500, title=\"American Revolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short answer  Q2\n",
    "Look at the visualization above and write a few sentences about your intuition on who in this network might be influential. Do any nodes have more connections than others? Do any nodes act as bridges by connecting multiple communities? Are any nodes closely connected to the entire network? How might each of these three properties relate to an individuals role in a social movement such as the American Revolution?\n",
    "\n",
    "(answer here ~150 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Find centrality\n",
    "Now let's move on to examine the centralities. The next cell uses `networkx` to calculate node centralities and then stores them in a data frame. For each node we also calculate a mean centrality value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"id\": G.nodes(), \"label\": G.nodes()}).set_index(\"id\")\n",
    "df['degree'] = pd.Series(nx.degree_centrality(G))\n",
    "df['betweenness'] = pd.Series(nx.betweenness_centrality(G))\n",
    "df['closeness'] = pd.Series(nx.closeness_centrality(G))\n",
    "df['eigenvector'] = pd.Series(nx.eigenvector_centrality(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean of all centrality measures\n",
    "df['mean_centrality'] = (\n",
    "    scale(df['degree'])\n",
    "    + scale(df['betweenness'])\n",
    "    + scale(df['closeness'])\n",
    "    + scale(df['eigenvector'])) / 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Degree\n",
    "\n",
    "One very simple way to find important nodes is to count how many neighbors they have.\n",
    "This measure is called the degree centrality.\n",
    "This number is typically divided by the total number of other nodes in the network, so a value\n",
    "of 0.82 means that a node is connected to 82% of the other nodes.\n",
    "The next cell shows the nodes with the highest degree centralities.\n",
    "\n",
    "Note that some of the people have the same degree. In fact, if two people have exactly the same set of neighbors,\n",
    "all of their centrality scores will be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sort_values('degree', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CUT THESE?__\n",
    "\n",
    "Instead of searching for the highest degree centrality nodes, we can find nodes that have an unusually high degree centrality compared to its other centralities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_centrality(df, 'degree').sort_values('rel_degree', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Betweenness\n",
    "\n",
    "Rather than highly-connected nodes, you might want to find nodes that connect different parts of the network.\n",
    "These types of nodes are sometimes called bridges, or brokers.\n",
    "The betweenness centrality is based on finding the shortest path between nodes.\n",
    "The nodes on that path play the role of bridges, connecting the endpoints.\n",
    "So the betweenness is the fraction of all shortest paths in the network that pass through a given node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('betweenness', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_centrality(df, 'betweenness').sort_values('rel_betweenness', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Closeness\n",
    "\n",
    "A nodes might also be considered important if it is close to many other nodes.\n",
    "In other words, if the paths connecting it to other nodes are all very short.\n",
    "This measure is called closeness centrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('closeness', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_centrality(df, 'closeness').sort_values('rel_closeness', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Eigenvector\n",
    "\n",
    "The last centrality measure covered in this section measures not just how well-connected a node is, but how well-connected its neighbors are as well.\n",
    "This measure is the eigenvector centrality.\n",
    "The PageRank algorithm used by Google to find high quality websites is an extension of this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.sort_values('eigenvector', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rel_centrality(df, 'eigenvector').sort_values('rel_eigenvector', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short Answer Q3\n",
    "\n",
    "Looking over the above centrality results, write a few sentences about how the different centrality measures compare. It may be helpful to see the list of organizations a person belonged to, which can be done in the next cell. Things to consider: which centrality measures give similar lists of individuals, whether the highest-ranked individuals make sense (and why), and how the lists of high-centrality individuals compare to your predictions from \n",
    "\n",
    "(answer ~150 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nx.neighbors(B_revere, \"Collier.Gershom\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare centrality measures\n",
    "\n",
    "The following plot shows how different centrality measures are related to each other (in this network).\n",
    "Each graph compares two centrality measures corresponding to its row and column.\n",
    "In each graph, each point represents a single node in the network.\n",
    "\n",
    "Points are drawn with transparency so overlapping points are darker.\n",
    "If a group of people have the same centralities,\n",
    "possibly because they have the same set of neighbors,\n",
    "they appear as a single dark spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store column names in array to keep order consistent\n",
    "measures = [\"degree\", \"betweenness\", \"eigenvector\", \"closeness\"]\n",
    "plt.figure(figsize=(7,6))\n",
    "# Loop through rows and columns\n",
    "# Even though we have 4 measures, we only need 3x3 to compare all\n",
    "for row in range(3):\n",
    "    for col in range(3):\n",
    "        # Each row column pair only needs to be plotted once\n",
    "        if row >= col:\n",
    "            # The longest row should correspond to the measure that\n",
    "            # does not appear on a column.\n",
    "            x, y = df[measures[row]], df[measures[(col-1) % 4]]\n",
    "            plt.subplot(3,3,1 + row*3 + col)\n",
    "            plt.plot(x, y, '.', alpha=0.3, markersize=10)\n",
    "            plt.xlim([0, x.max()]); plt.ylim([0, y.max()])\n",
    "        if row == 2:\n",
    "            plt.xlabel(measures[(col - 1) % 4])\n",
    "        if col == 0:\n",
    "            plt.ylabel(measures[row])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your social network\n",
    "Now you can do these analyses on your own social network! Specifically, you can download a list of all of your friends and whether they are friends with each other.\n",
    "This network is called your \"ego network.\"\n",
    "\n",
    "If you use Facebook, you can use the LostCircles plugin for Chrome to download your social network.\n",
    "1. Open the following link in Chrome to install LostCircles:\n",
    "https://chrome.google.com/webstore/detail/lost-circles-facebook-gra/ehpmfdlcppenimpibdifodjgfafkjhjl?hl=en\n",
    "2. Click the LostCircles icon on the Chrome toolbar and select \"Start Loading.\"\n",
    "3. Wait until the menu shows \"100%\" next to \"Start Loading.\"\n",
    "4. Click the LostCircles icon and then \"Download...\" followed by \"JSON (no pics)\". Save the result as `external/LostCircles/egonet.json`.\n",
    "5. Continue to the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def load_lost_circles_json(in_file, name=False):\n",
    "    with open(in_file) as f:\n",
    "        raw = json.load(f)\n",
    "    id_to_name = dict((i, datum[\"name\"]) for i, datum in enumerate(raw['nodes']))\n",
    "    if name:\n",
    "        edges = [(id_to_name[datum[\"source\"]], id_to_name[datum[\"target\"]], {\"capacity\":1}) for datum in raw['links']]\n",
    "    else:\n",
    "        edges = [(datum[\"source\"], datum[\"target\"], {\"capacity\":1}) for datum in raw['links']]\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edges)\n",
    "    return G, id_to_name, edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize your social network\n",
    "Let's start by visualizing your ego network.\n",
    "* Does it look like there are distinct groups? If so, can you explain why those groups exist?\n",
    "* Is everyone connected? Or are there disconnected components of the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G, id_to_name, edges = load_lost_circles_json(\"external/LostCircles/egonet.json\")\n",
    "visualize_visjs(G, title=\"Ego Net\", scale=6000, labels=id_to_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social network centrality\n",
    "Now you can repeat the centrality analysis for members of your own ego network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate centrality measures\n",
    "df = pd.DataFrame({\"id\": G.nodes(), \"label\": [id_to_name[x] for x in G.nodes()]}).set_index(\"id\")\n",
    "df['degree'] = pd.Series(nx.degree_centrality(G))\n",
    "df['betweenness'] = pd.Series(nx.betweenness_centrality(G))\n",
    "df['closeness'] = pd.Series(nx.closeness_centrality(G))\n",
    "df['eigenvector'] = pd.Series(nx.eigenvector_centrality(G))\n",
    "# Calculate mean of all centrality measures\n",
    "df['mean_centrality'] = (\n",
    "    scale(df['degree'])\n",
    "    + scale(df['betweenness'])\n",
    "    + scale(df['closeness'])\n",
    "    + scale(df['eigenvector'])) / 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree\n",
    "These are the most highly connected members of your network. Do they have anything in common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('degree', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_centrality(df, 'degree', labels=id_to_name).sort_values('rel_degree', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Betweenness\n",
    "These are the individuals who connect different parts of your network. Can you think of reasons these particular individuals might play that role?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('betweenness', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_centrality(df, 'betweenness', labels=id_to_name).sort_values('rel_betweenness', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closeness\n",
    "These individuals are, on average, closest to everyone you know. Do the results make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('closeness', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_centrality(df, 'closeness', labels=id_to_name).sort_values('rel_closeness', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvector\n",
    "These individuals are most connected to highly connected individuals. How do they compare to those identified by other methods? Can you explain why these individuals would have high centrality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('eigenvector', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_centrality(df, 'eigenvector', labels=id_to_name).sort_values('rel_eigenvector', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "\n",
    "* Did any of the measures show the same people as being most central?\n",
    "* Members of the Tea Party (e.g. `Collier.Gershom`) have a high degree centrality, but low betweenness centrality. What does that tell you about the Tea Party as compared to other groups?\n",
    "* What would a group with _high_ betweenness and _low_ degree centrality look like?\n",
    "* Of the important people you identified from the visualization, were any in the lists of high-centrality individuals? If so, which types of centrality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Healey2013] Healy, K. 2013. \"Using Metadata to Find Paul Revere.\" https://kieranhealy.org/blog/archives/2013/06/09/using-metadata-to-find-paul-revere/\n",
    "\n",
    "[CNM2004] Clauset, A., Newman, M. E., & Moore, C. (2004). Finding community structure in very large networks. Physical review E, 70(6).\n",
    "\n",
    "[RLWGBF2017] Rosenthal, S. B., Len, J., Webster, M., Gary, A., Birmingham, A., & Fisch, K. M. (2017). Interactive network visualization in Jupyter notebooks: visJS2jupyter. Bioinformatics.\n",
    "\n",
    "[BS2016] A. Beveridge and J. Shan, \"Network of Thrones,\" Math Horizons Magazine, Vol. 23, No. 4 (2016), pp. 18-22\n",
    "\n",
    "[Breiger1974] Breiger, R. L. (1974). The duality of persons and groups. Social forces, 53(2), 181-190.\n",
    "\n",
    "[Granovetter1977] Granovetter, M. S. (1977). The strength of weak ties. In Social networks (pp. 347-367)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
